{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution plot for each rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict as ddict\n",
    "order_l = [\"conclusion_first\",\"premise_first\"]\n",
    "available_models = [\"gpt\"]\n",
    "concat_file_name = \"concat.json\"\n",
    "max_node_file_name = \"max_node_len.json\"\n",
    "tail_head_list_template = [\"all_tail_{rule_index}_nodelen_max\",\"all_head_{rule_index}_nodelen_max\"]\n",
    "all_concat_data = ddict(lambda : ddict(lambda : ddict( dict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_search = \"path_to_likelihood_results_of_link\"\n",
    "path_baseline_gpt35 = \"path_to_likelihood_results_of_chatgpt\"\n",
    "path_baseline_gpt4 = \"path_to_likelihood_results_of_gpt4\"\n",
    "\n",
    "is_search_or_baseline_list = [\"search_rule\",\"baseline_gpt3.5_rule\",\"baseline_gpt4_rule\"]\n",
    "is_search_or_baseline_list = [\"LINK\",\"ChatGPT\",\"GPT4\"]\n",
    "search_or_baseline_path = [path_search,path_baseline_gpt35,path_baseline_gpt4]\n",
    "\n",
    "all_rule_keys = {\n",
    "    # \"conclusion_first\": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
    "    # \"premise_first\": [2, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 69, 70, 71, 72, 73, 74, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 130, 131, 132, 133, 134, 135]\n",
    "    \"conclusion_first\": [],\n",
    "    \"premise_first\": [65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]\n",
    "}\n",
    "\n",
    "\n",
    "for index,path in enumerate(search_or_baseline_path):\n",
    "\n",
    "    is_search_or_baseline = is_search_or_baseline_list[index]\n",
    "\n",
    "    if 1:\n",
    "        for order in order_l:\n",
    "            concat_file = os.path.join(os.path.join(path,order),concat_file_name)\n",
    "            max_node_file = os.path.join(os.path.join(path,order),max_node_file_name)\n",
    "            with open(max_node_file) as f:\n",
    "                rule_keys = list(json.load(f).keys())\n",
    "            with open(concat_file) as f:\n",
    "                concat_data = json.load(f)\n",
    "            rule_keys = [f\"rule{id}\" for id in all_rule_keys[order]]\n",
    "            for rule_key in rule_keys:\n",
    "                try:\n",
    "                    tail_head_list = [_.format(rule_index = rule_key) for _ in tail_head_list_template]\n",
    "                    all_concat_data[is_search_or_baseline][rule_key][\"tail\"][available_models[0]] = concat_data[available_models[0]][tail_head_list[0]]\n",
    "                    all_concat_data[is_search_or_baseline][rule_key][\"head\"][available_models[0]] = concat_data[available_models[0]][tail_head_list[1]]\n",
    "                except:\n",
    "                    print(path, order, rule_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/108 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:28<00:00,  3.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for rule in tqdm(all_concat_data[\"LINK\"]):\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    # 给定的三组列表数据\n",
    "    list1 = all_concat_data[\"LINK\"][rule][\"tail\"][\"gpt\"]\n",
    "    list2 = all_concat_data[\"ChatGPT\"][rule][\"tail\"][\"gpt\"]\n",
    "    list3 = all_concat_data[\"GPT4\"][rule][\"tail\"][\"gpt\"]\n",
    "    hist_all = list1 + list2 + list3\n",
    "\n",
    "    xmin_tail = np.min([np.min(arr) for arr in hist_all]) - 0\n",
    "    xmax_tail = np.max([np.max(arr) for arr in hist_all]) + 0\n",
    "\n",
    "\n",
    "    # ************************************************\n",
    "\n",
    "    list4 = all_concat_data[\"LINK\"][rule][\"head\"][\"gpt\"]\n",
    "    list5 = all_concat_data[\"ChatGPT\"][rule][\"head\"][\"gpt\"]\n",
    "    list6 = all_concat_data[\"GPT4\"][rule][\"head\"][\"gpt\"]\n",
    "    hist_all = list4 + list5 + list6\n",
    "\n",
    "    xmin_head = np.min([np.min(arr) for arr in hist_all]) - 0\n",
    "    xmax_head = np.max([np.max(arr) for arr in hist_all]) + 0\n",
    "\n",
    "    x_min = min(xmin_tail,xmin_head)\n",
    "    x_max = max(xmax_tail,xmax_head)\n",
    "\n",
    "\n",
    "    # ************************************************\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Subplot 1\", \"Subplot 2\"))\n",
    "\n",
    "\n",
    "    # 第一个子图的三个直方图\n",
    "    trace1 = go.Histogram(x=list1, opacity=0.5, name=\"LINK\",xbins=dict(size=0.02),marker=dict(color=colors[0]))\n",
    "    trace2 = go.Histogram(x=list2, opacity=0.5, name=\"ChatGPT\",xbins=dict(size=0.02),marker=dict(color=colors[1]))\n",
    "    trace3 = go.Histogram(x=list3, opacity=0.5, name=\"GPT4\",xbins=dict(size=0.02),marker=dict(color=colors[2]))\n",
    "\n",
    "\n",
    "\n",
    "    # 第二个子图的三个直方图\n",
    "    trace4 = go.Histogram(x=list4, opacity=0.5, name=\"LINK\",xbins=dict(size=0.02),marker=dict(color=colors[0]), showlegend=False)\n",
    "    trace5 = go.Histogram(x=list5, opacity=0.5, name=\"ChatGPT\",xbins=dict(size=0.02),marker=dict(color=colors[1]), showlegend=False)\n",
    "    trace6 = go.Histogram(x=list6, opacity=0.5, name=\"GPT4\",xbins=dict(size=0.02),marker=dict(color=colors[2]), showlegend=False)\n",
    "\n",
    "\n",
    "\n",
    "    fig.add_trace(trace1,row=1, col=1)\n",
    "    fig.add_trace(trace2,row=1, col=1)\n",
    "    fig.add_trace(trace3,row=1, col=1)\n",
    "    fig.add_trace(trace4,row=2, col=1)\n",
    "    fig.add_trace(trace5,row=2, col=1)\n",
    "    fig.add_trace(trace6,row=2, col=1)\n",
    "\n",
    "    # 对齐x axes\n",
    "    fig.update_xaxes(range=[x_min, x_max])\n",
    "\n",
    "\n",
    "\n",
    "    # 更新布局\n",
    "    fig.update_layout(\n",
    "        barmode=\"overlay\",  # 设置直方图模式为 overlay 以实现重叠效果\n",
    "        title_text=f\"{rule}\",  # 设置总标题\n",
    "        xaxis=dict(title=\"Value\"),\n",
    "        yaxis=dict(title=\"Frequency\"),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        annotations=[\n",
    "            dict(text=\"Long-tail Distribution\", x=0.5, xref=\"paper\", y=1.0, yref=\"paper\", showarrow=False, font_size=15),\n",
    "            dict(text=\"Head Distribution\", x=0.5, xref=\"paper\", y=0.4, yref=\"paper\", showarrow=False, font_size=15)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig['layout']['yaxis2'].update(title_standoff=40) \n",
    "\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Likelihood from text-davinci-003\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Likelihood from text-davinci-003\", row=2, col=1)\n",
    "    # 显示图表\n",
    "    # fig.show()\n",
    "    fig.write_image(f\"{rule}.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only use correct beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_use_correct_beams=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_l = [\"conclusion_first\",\"premise_first\"]\n",
    "available_models = [\"gpt\"]\n",
    "concat_file_name = \"concat.json\"\n",
    "max_node_file_name = \"max_node_len.json\"\n",
    "tail_head_list_template = [\"all_tail_{rule_index}_nodelen_max\",\"all_head_{rule_index}_nodelen_max\"]\n",
    "all_concat_data = ddict(lambda : ddict(lambda : ddict( lambda : ddict(list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_search = \"path_to_likelihood_results_of_link\"\n",
    "path_baseline_gpt35 = \"path_to_likelihood_results_of_chatgpt\"\n",
    "path_baseline_gpt4 = \"path_to_likelihood_results_of_gpt4\"\n",
    "\n",
    "accept_beam_file = [\n",
    "\t\"path_to_human_eval_results_of_link/accept_beams.json\",\n",
    "\t\"path_to_human_eval_results_of_chatgpt/accept_beams.json\",\n",
    "\t\"path_to_human_eval_results_of_gpt4/accept_beams.json\"\n",
    "\t]\n",
    "wrong_beam_file = [\n",
    "\t\"path_to_human_eval_results_of_link/all_wrong_beams.json\",\n",
    "\t\"path_to_human_eval_results_of_link/all_wrong_beams.json\",\n",
    "\t\"path_to_human_eval_results_of_link/all_wrong_beams.json\"\n",
    "]\n",
    "method_name_list = [\"new_search\", \"new_baseline_chatgpt\", \"new_baseline_gpt4\"]\n",
    "\n",
    "is_search_or_baseline_list = [\"search_rule\",\"baseline_gpt3.5_rule\",\"baseline_gpt4_rule\"]\n",
    "is_search_or_baseline_list = [\"LINK\",\"ChatGPT\",\"GPT4\"]\n",
    "search_or_baseline_path = [path_search,path_baseline_gpt35,path_baseline_gpt4]\n",
    "\n",
    "all_rule_keys = {\n",
    "    # \"conclusion_first\": [15],\n",
    "    \"conclusion_first\": [0],\n",
    "    # \"premise_first\": [2, 29, 69, 88]\n",
    "    \"premise_first\": [32, 46, 88, 112, 122]\n",
    "}\n",
    "\n",
    "\n",
    "for index,path in enumerate(search_or_baseline_path):\n",
    "\n",
    "    is_search_or_baseline = is_search_or_baseline_list[index]\n",
    "\n",
    "    if 1:\n",
    "        for order in order_l:\n",
    "            max_node_file = os.path.join(os.path.join(path,order),max_node_file_name)\n",
    "            concat_file = os.path.join(os.path.join(path,order),concat_file_name)\n",
    "            max_node_file = os.path.join(os.path.join(path,order),max_node_file_name)\n",
    "            with open(max_node_file) as f:\n",
    "                rule_keys = list(json.load(f).keys())\n",
    "            with open(concat_file) as f:\n",
    "                concat_data = json.load(f)\n",
    "            with open(max_node_file) as f:\n",
    "                node_max_len = json.load(f)\n",
    "            rule_keys = [f\"rule{id}\" for id in all_rule_keys[order]]\n",
    "            method_name = method_name_list[index]\n",
    "            with open(accept_beam_file[index], \"r\") as f:\n",
    "                accept_beam = json.load(f)[method_name]\n",
    "            with open(wrong_beam_file[index], \"r\") as f:\n",
    "                wrong_beam = json.load(f)[method_name]\n",
    "            for rule_key in rule_keys:\n",
    "                if 1:\n",
    "                    tail_head_list = [_.format(rule_index = rule_key) for _ in tail_head_list_template]\n",
    "                    for distribution in [\"head\", \"longtail\"]:\n",
    "                        dist = \"head\" if distribution == \"head\" else \"tail\"\n",
    "                        num = len(concat_data[available_models[0]][f\"all_{dist}_{rule_key}_nodelen_max\"])\n",
    "                        for i in range(num):\n",
    "                            if str(i) in accept_beam[rule_key][distribution]:\n",
    "                                label = \"correct\"\n",
    "                            elif str(i) in wrong_beam[rule_key][distribution]:\n",
    "                                label = \"incorrect\"\n",
    "                            else:\n",
    "                                continue\n",
    "                            if only_use_correct_beams:\n",
    "                                if label != \"correct\":\n",
    "                                    continue\n",
    "                            all_concat_data[is_search_or_baseline][rule_key][dist][available_models[0]].append(concat_data[available_models[0]][f\"{rule_key}_{dist}_{i}_{node_max_len[rule_key]}\"][0])     \n",
    "                # except Exception as e:\n",
    "                #     print(path, order, rule_key, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for rule in tqdm(all_concat_data[\"LINK\"]):\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    # 给定的三组列表数据\n",
    "    list1 = all_concat_data[\"LINK\"][rule][\"tail\"][\"gpt\"]\n",
    "    list2 = all_concat_data[\"ChatGPT\"][rule][\"tail\"][\"gpt\"]\n",
    "    list3 = all_concat_data[\"GPT4\"][rule][\"tail\"][\"gpt\"]\n",
    "    hist_all = list1 + list2 + list3\n",
    "\n",
    "    xmin_tail = np.min([np.min(arr) for arr in hist_all]) - 0.1\n",
    "    xmax_tail = np.max([np.max(arr) for arr in hist_all]) + 0.1\n",
    "\n",
    "\n",
    "    # ************************************************\n",
    "\n",
    "    list4 = all_concat_data[\"LINK\"][rule][\"head\"][\"gpt\"]\n",
    "    list5 = all_concat_data[\"ChatGPT\"][rule][\"head\"][\"gpt\"]\n",
    "    list6 = all_concat_data[\"GPT4\"][rule][\"head\"][\"gpt\"]\n",
    "    hist_all = list4 + list5 + list6\n",
    "\n",
    "    xmin_head = np.min([np.min(arr) for arr in hist_all]) - 0.1\n",
    "    xmax_head = np.max([np.max(arr) for arr in hist_all]) + 0.1\n",
    "\n",
    "    x_min = min(xmin_tail,xmin_head)\n",
    "    x_max = max(xmax_tail,xmax_head)\n",
    "\n",
    "\n",
    "    # ************************************************\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Long-tail Distribution\", \"Head Distribution\"))\n",
    "\n",
    "\n",
    "    # 第一个子图的三个直方图\n",
    "    trace1 = go.Histogram(x=list1, opacity=0.5, name=\"LINK\",xbins=dict(size=0.02),marker=dict(color=colors[0]))\n",
    "    trace2 = go.Histogram(x=list2, opacity=0.5, name=\"ChatGPT\",xbins=dict(size=0.02),marker=dict(color=colors[1]))\n",
    "    trace3 = go.Histogram(x=list3, opacity=0.5, name=\"GPT4\",xbins=dict(size=0.02),marker=dict(color=colors[2]))\n",
    "\n",
    "\n",
    "\n",
    "    # 第二个子图的三个直方图\n",
    "    trace4 = go.Histogram(x=list4, opacity=0.5, name=\"LINK\",xbins=dict(size=0.02),marker=dict(color=colors[0]), showlegend=False)\n",
    "    trace5 = go.Histogram(x=list5, opacity=0.5, name=\"ChatGPT\",xbins=dict(size=0.02),marker=dict(color=colors[1]), showlegend=False)\n",
    "    trace6 = go.Histogram(x=list6, opacity=0.5, name=\"GPT4\",xbins=dict(size=0.02),marker=dict(color=colors[2]), showlegend=False)\n",
    "\n",
    "\n",
    "\n",
    "    fig.add_trace(trace1,row=1, col=1)\n",
    "    fig.add_trace(trace2,row=1, col=1)\n",
    "    fig.add_trace(trace3,row=1, col=1)\n",
    "    fig.add_trace(trace4,row=2, col=1)\n",
    "    fig.add_trace(trace5,row=2, col=1)\n",
    "    fig.add_trace(trace6,row=2, col=1)\n",
    "\n",
    "    # 对齐x axes\n",
    "    fig.update_xaxes(range=[x_min, x_max])\n",
    "\n",
    "    for annotation in fig['layout']['annotations']: \n",
    "        annotation['font'] = dict(size=20)  # 设置字体大小\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Likelihood distribution for <b>{rule}</b> over <b>text-davinci-003</b>\",\n",
    "        barmode='overlay',\n",
    "        title_font=dict(size=21)\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(row=1, col=1, title_text=\"Likelihood from text-davinci-003\", title_font=dict(size=20))  # 用你的实际范围替换 xmin 和 xmax\n",
    "    fig.update_xaxes(row=2, col=1, title_text=\"Likelihood from text-davinci-003\", title_font=dict(size=20)) \n",
    "    fig.update_yaxes(row=1, col=1, range=[0, 15],title_text=\"# of statements\", title_font=dict(size=17.5)) \n",
    "    fig.update_yaxes(row=2, col=1, range=[0, 15],title_text=\"# of statements\", title_font=dict(size=17.5)) \n",
    "\n",
    "    # 显示图表\n",
    "    fig.update_layout(\n",
    "        legend_font=dict(size=15)         \n",
    "    )\n",
    "\n",
    "    import plotly.io as pio\n",
    "    img_bytes = pio.to_image(fig, format=\"pdf\", scale=7,width=800, height=600)\n",
    "    with open(f\"{rule}.pdf\", \"wb\") as f:\n",
    "        f.write(img_bytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
